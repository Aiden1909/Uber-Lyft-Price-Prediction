---
title: "R Notebook"
output: html_notebook
---

# Feature Selection

We first remove unnecessary columns like `id` and `product_id` and also remove `source` and `destination` because `distance` already accounts for the relationship between the starting point and the destination.

```{r}
library(dplyr)
training_data <- read.csv('training_data.csv')
test_data <- read.csv(('test_data.csv'))

training_data <- training_data %>% select(-id, -product_id)
test_data <- test_data %>% select(-id, -product_id)
```

```{r}
str(training_data)
```

First we would convert all category variables to factor data type

```{r}
training_data$cab_type <- as.factor(training_data$cab_type)
training_data$name <- as.factor(training_data$name)
training_data$time_period <- as.factor(training_data$time_period)
training_data$day_of_week <- as.factor(training_data$day_of_week)
training_data$source <- as.factor(training_data$source)
training_data$destination <- as.factor(training_data$destination)

str(training_data)
```

```{r}
par(mfrow = c(2, 2))  # 2x2 layout
subset1 <- training_data[, c("price", "distance", "surge_multiplier")]
pairs(subset1, main = "Subset 1: Price, Distance, Surge Multiplier")

# Subset 2: Numerical weather-related predictors
subset2 <- training_data[, c("price", "temp", "clouds", "rain", "humidity", "wind")]
pairs(subset2, main = "Subset 2: Price, Temp, Clouds, Rain, Humidity, Wind")

# Subset 3: Cab types and ride-specific predictors
subset3 <- training_data[, c("price", "cab_type", "name")]
pairs(subset3, main = "Subset 3: Price, Cab Type, Name")

# Subset 4: Time and day-related predictors
subset4 <- training_data[, c("price", "peak_hour", "weekend", "day_of_week")]
pairs(subset4, main = "Subset 4: Price, Peak Hour, Weekend, Day of Week")
```

### Observations from the Plots:

#### Observations from Pairwise Scatterplots:

1.  **Subset 1: Price, Distance, Surge Multiplier**

    -   A clear **positive relationship** is observed between `price` and `distance`.

    -   **Surge multiplier** also shows a noticeable effect on price, with higher surge multipliers correlating with higher prices.

2.  **Subset 2: Price, Temp, Clouds, Rain, Humidity, Wind**

    -   **Weather variables (temp, clouds, humidity, rain, wind)** seem to have weaker relationships with `price`.

    -   Slight variability can be seen for `rain`, indicating that rainy conditions might influence prices to a small extent.

3.  **Subset 3: Price, Cab Type, Name**

    -   **Ride types (`name`)** show distinct price distributions, with clear separation among categories (e.g., economy vs. luxury).

    -   **Cab type (`Lyft` vs. `Uber`)** also displays visible differentiation in price ranges.

4.  **Subset 4: Price, Peak Hour, Weekend, Day of Week**

    -   Both **`peak_hour`**, **`weekend`** and **`day_of_week`** seem to have subtle effects on price, though the trends are not strongly evident in the scatterplots.

Training on full model first

```{r}
model <- lm(price ~ ., data = training_data)
summary(model)
```

### Key Insights:

#### **1. Model Fit**

-   **R² (0.9336)**: 93.36% of the variation in `price` is explained by the predictors.

-   **Adjusted R² (0.9259)**: Adjusted for the number of predictors, it remains very high, suggesting that most predictors are contributing meaningfully.

-   **Residual Standard Error (2.464)**: The average deviation of observed prices from predicted prices is around 2.46 units.

#### **2. Significant Predictors**

-   Predictors with p-values less than 0.05 are statistically significant:

    -   **`distance`**: Highly significant (**p \< 2e-16**), with an increase of approximately **\$2.45** per unit of distance.

    -   **`cab_typeUber`**: Uber rides are **\$14.50** more expensive on average compared to Lyft (**p \< 2e-16**).

    -   **`surge_multiplier`**: Each unit increase in surge multiplier adds **\$20.74** to the price (**p \< 2e-16**).

    -   **`name` (ride types)**:

        -   Significant across categories, e.g., **"Lux Black XL"** adds **\$25.64**, while **"UberPool"** reduces price by **\$12.14**.

    -   **`destinationFinancial District`** and **`destinationNorth Station`**: Both are associated with significant price differences.

    -   **`time_periodnight`**: Prices are lower by **\$1.24** during nighttime hours (**p = 0.0133**).

#### **3. Insignificant Predictors**

-   Variables with p-values \> 0.05 do not significantly contribute to the model:

    -   **Source and Destination (most levels)**: Many levels are insignificant, likely due to redundancy or overlap in capturing price variability.

    -   **Weather Variables**: `temp`, `clouds`, `pressure`, `rain`, `wind`, and `humidity` are mostly insignificant.

    -   **Time Periods (other than night)**: Most time periods like `evening`, `late_night`, and `morning` are not significant.

    -   **Days of the Week**: Insignificant, suggesting that day-to-day variability in pricing is minimal.

#### **4. Singularities (NA Coefficients)**

-   Some variables (e.g., `destinationSouth Station`, `nameShared`, `weekend`) are excluded from the model due to **collinearity**. This means these variables are perfectly correlated with others and do not provide additional information.

### Phase 1: Best Subset of Predictors using AIC & BIC

We can see from the above summary that `distance`, `surge_multiplier` , `name` and few categories of `destination` and `time_period` seem to be the important predictors with 95% confidence interval. Next, we want to confirm the best predictors by getting the best AIC & BIC using backward selection.

##### AIC

```{r}
# Perform backward selection using step() with AIC
step(model, direction = "backward", trace = 0)
```

```{r}
aic_model = lm(formula = price ~ distance + surge_multiplier + name + rain + 
    peak_hour, data = training_data)

summary(aic_model)
```

### Key Results:

1.  **Model Fit**:

    -   **Multiple R²: 0.9296**:

        -   The model explains 92.96% of the variability in `price`, which is excellent.

    -   **Adjusted R²: 0.9274**:

        -   Slightly lower due to the number of predictors, but still very strong.

    -   **Residual Standard Error (2.439)**:

        -   On average, the predicted prices deviate by \~2.44 units from the observed prices.

<!-- -->

2.  **Significant Predictors**:

    -   **`distance`**:

        -   Coefficient: 2.33 (**p \< 2e-16**).

        -   A \$2.33 increase in price for every unit increase in distance.

    -   **`surge_multiplier`**:

        -   Coefficient: 20.99 (**p \< 2e-16**).

        -   A 1-unit increase in surge multiplier adds \~\$21 to the price.

    -   **Ride Types (`name`)**:

        -   All categories are significant, with wide-ranging effects on price:

            -   Positive: `Black SUV` (+\$10.17), `Lux Black XL` (+\$11.26).

            -   Negative: `Lyft` (-\$11.99), `UberPool` (-\$12.24).

        -   This highlights the strong influence of ride type on price.

    -   **`rain`**:

        -   Coefficient: 3.70 (**p = 0.0754**).

        -   Prices tend to increase by \~\$3.70 during rain (weakly significant at the 10% level).

    -   **`peak_hour`**:

        -   Coefficient: -0.54 (**p = 0.065**).

        -   Weak evidence that prices decrease slightly during peak hours.

<!-- -->

3.  **Excluded Predictors**:

    -   Stepwise AIC excluded variables like:

        -   `source` and `destination`: Likely redundant or insignificant after including `distance`.

        -   Weather variables (`temp`, `clouds`, etc.): Likely not strong predictors of price.

        -   `time_period` and `day_of_week`: Weak contributors to variability in price.

### Insights from the Model:

-   **Key Drivers of Price**:

    -   `distance` and `surge_multiplier` are the most impactful predictors, as expected.

    -   Ride types (`name`) capture pricing tiers and account for substantial variability.

    -   Weather (`rain`) has a small but noticeable effect, aligning with real-world surge pricing practices during inclement weather.

-   **Peak Hour Dynamics**:

    -   The weak negative coefficient for `peak_hour` suggests that peak hours might have been indirectly captured through other variables (e.g., `surge_multiplier`).

##### BIC

```{r}
step(model, direction = "backward", trace = 0, k = log(nrow(training_data)))
```

```{r}
bic_model = lm(formula = price ~ distance + surge_multiplier + name, data = training_data)
summary(bic_model)
```

### Key Results:

1.  **Model Fit**:

    -   **Multiple R²: 0.9287**:

        -   The model explains 92.87% of the variability in `price`, which is excellent.

    -   **Adjusted R²: 0.9268**:

        -   Adjusted for the number of predictors, it remains very strong, indicating good generalizability.

    -   **Residual Standard Error (2.449)**:

        -   The average deviation of predicted prices from observed prices is \~2.45 units, consistent with the previous models.

<!-- -->

2.  **Significant Predictors**:

    -   **`distance`**:

        -   Coefficient: 2.34 (**p \< 2e-16**).

        -   A \$2.34 increase in price for every unit increase in distance.

    -   **`surge_multiplier`**:

        -   Coefficient: 20.94 (**p \< 2e-16**).

        -   Each 1-unit increase in surge multiplier adds \~\$20.94 to the price.

    -   **Ride Types (`name`)**:

        -   All levels are highly significant, reflecting their critical role in determining price:

            -   Positive Effects: `Black SUV` (+\$10.25), `Lux Black XL` (+\$11.25).

            -   Negative Effects: `UberPool` (-\$12.18), `Lyft` (-\$11.99).

        -   The coefficients highlight how different ride types contribute to price variability.

<!-- -->

3.  **Excluded Predictors**:

    -   Stepwise BIC removed variables like:

        -   `rain`, `peak_hour`: These contributed minimally to the variability in price compared to the remaining predictors.

        -   `source` and `destination`: Likely redundant after including `distance`.

### Interpretation of the Model:

-   **Compact and Interpretable**:

    -   The BIC-optimized model is simpler with only three predictors, making it easier to interpret and communicate results.

-   **Key Drivers of Price**:

    -   `distance` and `surge_multiplier` are the most impactful continuous predictors, as expected.

    -   `name` captures the pricing tiers across different ride types.

### Phase 2: Residual Plot Test

Both AIC and BIC models are promising, so we will compare them using residual and QQ plots and whichever follows the assumptions better will be taken as the best model.

##### AIC

```{r}
par(mfrow = c(1, 2))

plot(resid(aic_model)~fitted(aic_model), col = "grey", pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "Residual plot")
abline(h = 0, col = "darkorange", lwd = 2)

qqnorm(resid(aic_model), main = "Normal Q-Q Plot", col = "darkgrey")
qqline(resid(aic_model), col = "dodgerblue", lwd = 2)
```

##### BIC

```{r}
par(mfrow = c(1, 2))

plot(resid(bic_model)~fitted(bic_model), col = "grey", pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "Residual plot")
abline(h = 0, col = "darkorange", lwd = 2)

qqnorm(resid(bic_model), main = "Normal Q-Q Plot", col = "darkgrey")
qqline(resid(bic_model), col = "dodgerblue", lwd = 2)
```

### Comparison of Residual Plots for AIC and BIC Models

The residual diagnostics for the **BIC model** appear very similar to those for the **AIC model**:

1.  **Residual Plot**:

    -   The residuals are randomly scattered around the horizontal line at 0, which is good.

    -   Heteroscedasticity (slightly increasing spread for higher fitted values) is still visible, just as in the AIC model.

    -   No significant difference from the AIC model in this regard.

2.  **Normal Q-Q Plot**:

    -   The residuals align closely with the theoretical line, except for slight deviations in the tails.

    -   Again, this is similar to the AIC model.

### Decision Between AIC and BIC

Since the residual patterns for both models are similar, the decision comes down to the following:

-   **AIC**:

    -   Slightly more complex model with additional predictors.

    -   Better for predictive accuracy if you aim to maximize fit, especially for larger datasets.

-   **BIC**:

    -   Simpler model with fewer predictors.

    -   Prioritizes parsimony and interpretability, making it a better choice if you value simplicity without sacrificing much explanatory power.

Hence we will go with BIC model for simplicity as it does not sacrifice much predictive power, given that we are training on a small dataset.

Testing for significance of interaction

```{r}
interaction_model = lm(price ~ distance*surge_multiplier,training_data)
summary(interaction_model)
```

```{r}
anova(reduced_model,interaction_model)
```

1.  **F-Statistic and p-Value**:

    -   **F = 5.2789**, **p = 0.022**:

        -   The p-value is below 0.05, indicating the interaction term is statistically significant.

        -   This suggests that the relationship between `distance` and `price` depends on the value of `surge_multiplier`, and the interaction should be included in the model.

### Interpretation:

-   **Significant Interaction**:

    -   The interaction term (`distance * surge_multiplier`) captures how the effect of `distance` on `price` changes with `surge_multiplier`. For example:

        -   Longer distances may be disproportionately affected during surge pricing periods.

    -   This significantly improves the model's fit compared to the simpler model without the interaction.

```{r}
# Perform backward selection using step() with AIC
step(model, direction = "backward", trace = 0)
```

```{r}
step(model, direction = "backward", trace = 0, k = log(nrow(training_data)))
```

```{r}
aic_model = lm(formula = price ~ distance + surge_multiplier + name + rain + 
    peak_hour, data = training_data)
summary(aic_model)
```

```{r}
bic_model = lm(formula = price ~ distance + surge_multiplier + name, data = training_data)
summary(bic_model)
```
