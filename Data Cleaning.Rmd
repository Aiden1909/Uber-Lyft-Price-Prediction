---
title: "Uber and Lyft Cab Prices"
output:
  html_document:
    df_print: paged
---

## Loading the tables

```{r}
cab_df = read.csv('cab_rides.csv')

str(cab_df)
```

```{r}
weather_df = read.csv('weather.csv')
str(weather_df)
```

```{r}
View(cab_df)
```

## Data Cleaning

We see that the time-stamp columns are not uniform between the weather and the cab ride dataset so we will first convert them to proper datetime format.

```{r}
cab_df$time_stamp <- as.POSIXct(cab_df$time_stamp / 1000, origin = "1970-01-01", tz = "EST")
weather_df$time_stamp <- as.POSIXct(weather_df$time_stamp, origin = "1970-01-01", tz = "EST")
```

```{r}
head(weather_df)
```

Next we will try to find missing data across the datasets.

```{r}
# Check for missing values in cab_rides dataset
colSums(is.na(cab_df))
```

```{r}
# Check for missing values in weather dataset
colSums(is.na(weather_df))
```

We can see that we have 55095 missing values in price and 5382 in rain. We will fill the null values of rain with 0 because there are chances of not raining in a particular city at a particular time.

```{r}
weather_df$rain[is.na(weather_df$rain)] <- 0

# Verify that there are no missing values in 'rain'
sum(is.na(weather_df$rain))
```

Next we will round the timestamps to nearest hour and merge the datasets on location and timestamp.

```{r}
cab_df$time_stamp <- lubridate::floor_date(cab_df$time_stamp, unit = "hour")
weather_df$time_stamp <- lubridate::floor_date(weather_df$time_stamp, unit = "hour")

merged_data <- merge(cab_df, weather_df, by.x = c("time_stamp", "source"), by.y = c("time_stamp", "location"), all.x = TRUE)

str(merged_data)
```

And finally we will remove all null values to clean the data.

```{r}
cleaned_data <- na.omit(merged_data)

sum(is.na(cleaned_data))
```

Categorizing timestamp into times of day, weekends and day of week for additional interpretations

```{r}
# Extract hour from datetime
cleaned_data$hour <- format(cleaned_data$time_stamp, "%H")

# Define the mapping for time periods
time_period_mapping <- list(
  morning = c("06", "07", "08", "09"),
  noon = c("10", "11", "12", "13"),
  afternoon = c("14", "15", "16", "17"),
  evening = c("18", "19", "20", "21"),
  night = c("22", "23", "00", "01", "02"),
  late_night = c("03", "04", "05")
)

# Create time_period column based on the mapping
cleaned_data$time_period <- sapply(cleaned_data$hour, function(h) {
  period <- names(Filter(function(x) h %in% x, time_period_mapping))
  if (length(period) > 0) return(period) else return(NA)
})

cleaned_data$day_of_week <- weekdays(cleaned_data$time_stamp)
cleaned_data$weekend <- ifelse(cleaned_data$day_of_week %in% c("Saturday", "Sunday"), 1, 0)

cleaned_data$peak_hour <- ifelse(cleaned_data$hour %in% c(7:9, 17:19), 1, 0)

# Drop unnecessary columns
cleaned_data <- cleaned_data %>%
  select(-time_stamp, -hour)

# View the updated dataset
head(cleaned_data)
```

```{r}
head(cleaned_data)
```

For our project, we will will sample random 500 records for training and testing our regression model.

```{r}
set.seed(123)

# Randomly sample 500 rows for training
training_data <- cleaned_data[sample(nrow(cleaned_data), 500), ]

# Randomly sample another 500 rows for testing (ensure no overlap)
test_data <- cleaned_data[sample(nrow(cleaned_data), 500), ]
```

```{r}
print(dim(training_data))
print(dim(test_data))
```

```{r}
summary(training_data)
```

```{r}
summary(test_data)
```

```{r}
# Save the training dataset to a CSV file
write.csv(training_data, "training_data.csv", row.names = FALSE)

# Save the testing dataset to a CSV file
write.csv(test_data, "test_data.csv", row.names = FALSE)
```
