---
title: "R Notebook"
output: html_notebook
---

# Model Selection

```{r}
training_data <- read.csv('training_data.csv')
test_data <- read.csv(('test_data.csv'))

training_data$name <- as.factor(training_data$name)
```

### Residual Plots Comparison

We have finalized `distance` and `surge_multiplier` and `name` as are predictors for our model. Next we need to see if our data is able to satisfy all assumptions that are needed for fitting into a linear regression model.

```{r}
model = lm(price ~ distance + surge_multiplier + name,training_data)

par(mfrow = c(1, 2))

plot(resid(model)~fitted(model), col = "grey", pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "Residual plot")
abline(h = 0, col = "darkorange", lwd = 2)

qqnorm(resid(model), main = "Normal Q-Q Plot", col = "darkgrey")
qqline(resid(model), col = "dodgerblue", lwd = 2)
```

### Residual Plot:

-   **Scattered Residuals**:

    -   The residuals are randomly scattered around the horizontal line at 0, which is a good sign.

    -   However, there is some **heteroscedasticity** (slight funnel shape), where the spread of residuals increases with higher fitted values.

-   **No Major Patterns**:

    -   No clear non-linear patterns, indicating the model's functional form is appropriate.

### Normal Q-Q Plot:

-   **Alignment with the Line**:

    -   Residuals align well with the theoretical line, indicating approximate normality.

-   **Deviations in the Tails**:

    -   Some deviations are visible at the extreme ends, which could indicate a few outliers or non-normality in the tails.

### Observations:

-   The model performs well overall but could benefit from addressing the slight heteroscedasticity and deviations in the tails.

```{r}
library(lmtest)
library(stats)
bptest(model)
shapiro.test(residuals(model))
```

### Solution 1: Removing Outliers

First we check if our data contains any outliers. If there is, then we remove it and try to check for violations again.

```{r}
model_cd = cooks.distance(model)
influential_points = which(model_cd > 4/length(model_cd))
cat("Indices of influential points:", influential_points, "\n")

(training_data[influential_points, c('price','distance','surge_multiplier','name')])
```

### Observations:

1.  **Unusual Values in `distance` or `surge_multiplier`**:

    -   Some points have unusually high distances (e.g., 7.19 at row 255) or surge multipliers (e.g., 2.00 at row 149).

    -   These may skew the regression model.

2.  **Extreme Prices for Specific Ride Types**:

    -   Certain ride types (`UberPool`, `Lux Black XL`, `WAV`) appear multiple times with extreme prices.

3.  **Possible Data Entry Errors**:

    -   Some low `distance` values correspond to disproportionately high prices, which could indicate potential data issues.

```{r}
refined_data = training_data[-influential_points,] 
model1 = lm(price ~ distance + surge_multiplier + name, refined_data)

par(mfrow = c(1, 2))

plot(resid(model1)~fitted(model1), col = "grey", pch = 20, xlab = "Fitted", ylab = "Residuals", main = "Residual plot") 
abline(h = 0, col = "darkorange", lwd = 2)

qqnorm(resid(model1), main = "Normal Q-Q Plot", col = "darkgrey") 
qqline(resid(model1), col = "dodgerblue", lwd = 2)
```

### Analysis of Residual Plots After Removing Outliers

#### 1. **Residuals vs. Fitted Plot (Left)**:

-   **Random Scatter**:

    -   The residuals are more evenly scattered around the horizontal line at 0, indicating better model fit.

-   **Reduced Heteroscedasticity**:

    -   The spread of residuals appears more consistent across fitted values, showing that removing outliers helped address heteroscedasticity.

#### 2. **Normal Q-Q Plot (Right)**:

-   **Improved Normality**:

    -   The residuals align more closely with the theoretical Q-Q line, especially in the tails.

    -   This suggests the residuals now better approximate a normal distribution.

```{r}
print(bptest(model1))
print(shapiro.test(residuals(model1)))
```

#### 1. **Studentized Breusch-Pagan Test**:

-   **Result**:

    -   **BP Statistic = 72.328**, **p-value = 2.983e-10**.

    -   The null hypothesis of **homoscedasticity** (constant variance) is still rejected.

    -   Although heteroscedasticity has likely decreased compared to earlier results (BP = 151.49), it is still present.

-   **What This Means**:

    -   There is evidence that the variance of residuals is not constant.

    -   While the model is improved, additional steps may still be needed to address heteroscedasticity.

#### 2. **Shapiro-Wilk Normality Test**:

-   **Result**:

    -   **W = 0.99193**, **p-value = 0.01158**.

    -   The null hypothesis of **normality** is rejected (p-value \< 0.05).

    -   This indicates that the residuals are not perfectly normal, though the deviation may not be severe (W is very close to 1).

-   **What This Means**:

    -   The residuals are approximately normal, but some deviations (possibly in the tails) remain.

    -   This result aligns with the Q-Q plot, where the residuals showed slight deviations in the tails.

### Solution 2: Box-Cox Transformation

Next we go with box cox so that we get a optimal lambda value for our new function of `price`

```{r}
library(MASS)
par(mfrow=c(1,1))
boxcox(model, lambda = seq(-1, 1, by = 0.05))
```

From the plot, we can see 𝝺 = 0.25 approx has the highest log likelihood. So we will take log(price) as our target variable and then try to fit the model.

```{r}
lambda = 0.25
model_transf <- lm(((price^(lambda)-1)/(lambda))~distance+surge_multiplier+name,data=training_data)
par(mfrow = c(1, 2))

plot(resid(model_transf)~fitted(model_transf), col = "grey", pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "Residual plot")
abline(h = 0, col = "darkorange", lwd = 2)

qqnorm(resid(model_transf), main = "Normal Q-Q Plot", col = "darkgrey")
qqline(resid(model_transf), col = "dodgerblue", lwd = 2)
```

### Residual Diagnostics After Box-Cox Transformation (λ=0.25):

#### 1. **Residuals Plot** :

-   **Improvement in Homoscedasticity**:

    -   The residuals are now more uniformly scattered around the horizontal line at 0.

    -   The spread of residuals is more consistent across the range of fitted values, suggesting that heteroscedasticity has been mitigated.

-   **No Apparent Patterns**:

    -   There is no discernible pattern in the residuals, indicating the model is well-specified.

#### 2. **Normal Q-Q Plot** :

-   **Better Normality**:

    -   The residuals align more closely with the theoretical line, especially in the middle range.

    -   Slight deviations are still visible in the tails, but these are much less pronounced than before.

```{r}
print(bptest(model_transf))
print(shapiro.test(residuals(model_transf)))
```

#### 1. **Studentized Breusch-Pagan Test**:

-   **BP Statistic = 27.163, p-value = 0.01182**:

    -   The null hypothesis of **homoscedasticity** (constant variance) is **rejected** (p-value \< 0.05).

    -   However, the test statistic (BP = 27.163) is significantly lower than before (BP = 72.328), indicating a **reduction in heteroscedasticity** after the transformation.

-   **What This Means**:

    -   While the variance of residuals is not perfectly constant, the issue is less severe than in the original model.

    -   If further improvement is needed, consider **weighted least squares** or robust standard errors.

#### 2. **Shapiro-Wilk Normality Test**:

-   **W = 0.95907, p-value = 1.454e-10**:

    -   The null hypothesis of **normality** is rejected (p-value \< 0.05).

    -   The W statistic (closer to 1) indicates that the residuals are closer to normality compared to the untransformed model, but deviations remain.

-   **What This Means**:

    -   The Box-Cox transformation has improved normality, but some non-normality persists, particularly in the tails, as indicated by the Q-Q plot.

### Solution 3: 2nd Order

```{r}
model_quad <- lm(price ~ distance + surge_multiplier + I(distance^2) + I(surge_multiplier^2) + name, data = training_data)
summary(model_quad)
```

```{r}
par(mfrow = c(1, 2))

plot(resid(model_quad)~fitted(model_quad), col = "grey", pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "Residual plot")
abline(h = 0, col = "darkorange", lwd = 2)

qqnorm(resid(model_quad), main = "Normal Q-Q Plot", col = "darkgrey")
qqline(resid(model_quad), col = "dodgerblue", lwd = 2)
```

### Residual Diagnostics for the Quadratic Model

#### **1. Residual Plot (Left)**:

-   **Homoscedasticity**:

    -   The residuals are fairly evenly distributed around the horizontal line at zero.

    -   Some clustering is visible in the lower fitted value range, indicating potential slight heteroscedasticity.

-   **No Systematic Pattern**:

    -   There are no obvious non-linear patterns, suggesting the quadratic terms are addressing any curvature in the data.

#### **2. Normal Q-Q Plot (Right)**:

-   **Normality**:

    -   The residuals align closely with the theoretical line, particularly in the center range of the distribution.

    -   Slight deviations in the tails persist, but these are expected in most practical scenarios and may not significantly affect the model's performance.

```{r}
print(bptest(model_quad))
print(shapiro.test(residuals(model_quad)))
```

#### **1. Studentized Breusch-Pagan Test**:

-   **BP Statistic = 130.98, p-value \< 2.2e-16**:

    -   The null hypothesis of **homoscedasticity** (constant variance) is strongly rejected (p-value \< 0.05).

    -   The test indicates **severe heteroscedasticity** in the residuals.

-   **What This Means**:

    -   The quadratic model captures some non-linear relationships but fails to resolve variance instability effectively.

### Combining Outlier removal, 2nd order and BoxCox

We will combine all the three solutions to make use of all the advantages of these to stabilize the assumptions

```{r}

model_2nd_order <- lm(price ~ 
                      distance + I(distance^2) + 
                      surge_multiplier + I(surge_multiplier^2) + 
                      name, data = refined_data)
summary(model_2nd_order)
```

```{r}
boxcox(model_2nd_order, lambda = seq(-1, 1, 0.05))
```

Taking lambda=0.4

```{r}
lambda = 0.4
model_transf_2nd_order <- lm(((price^(lambda)-1)/(lambda))~distance + surge_multiplier + I(distance^2) + I(surge_multiplier^2) + name,data=refined_data)
par(mfrow = c(1, 2))

plot(resid(model_transf_2nd_order)~fitted(model_transf_2nd_order), col = "grey", pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "Residual plot")
abline(h = 0, col = "darkorange", lwd = 2)

qqnorm(resid(model_transf_2nd_order), main = "Normal Q-Q Plot", col = "darkgrey")
qqline(resid(model_transf_2nd_order), col = "dodgerblue", lwd = 2)
```

```{r}
print(bptest(model_transf_2nd_order))
print(shapiro.test(residuals(model_transf_2nd_order)))
```

### Interpretation of Residual Plots After Transformation and Second-Order Terms (λ=0.4):

#### 1. **Residual Plot**:

-   **Improved Homoscedasticity**:

    -   The residuals appear more evenly scattered around the horizontal line at 0, with no apparent pattern or funnel shape.

    -   This indicates that heteroscedasticity has been further mitigated.

-   **No Major Patterns**:

    -   The absence of clear patterns suggests the model is well-specified for the transformed response variable.

#### 2. **Normal Q-Q Plot**:

-   **Better Alignment with the Theoretical Line**:

    -   The residuals closely follow the blue reference line, especially in the middle range, indicating improved normality.

    -   Minor deviations are still observed in the tails, but they are significantly reduced compared to earlier models.

**Model Fit**:

-   The transformation (λ=0.4) and inclusion of second-order terms have significantly improved compliance with linear regression assumptions:

    -   Homoscedasticity: Residuals are more uniformly scattered.

    -   Normality: Residuals are closer to a normal distribution.

#### 1. **Studentized Breusch-Pagan Test**:

-   **Result**:

    -   **BP Statistic = 28.957**, **p-value = 0.01629**.

    -   The null hypothesis of **homoscedasticity** (constant variance) is rejected (p-value \< 0.05).

    -   However, the BP statistic and p-value indicate that **heteroscedasticity is much less severe** compared to earlier results.

-   **What This Means**:

    -   While some residual variance remains non-constant, the Box-Cox transformation and second-order terms have significantly mitigated heteroscedasticity.

    -   The residuals are now acceptable for many practical applications.

#### 2. **Shapiro-Wilk Normality Test**:

-   **Result**:

    -   **W = 0.99408**, **p-value = 0.06349**.

    -   The null hypothesis of **normality** is **not rejected** (p-value \> 0.05).

    -   This indicates that the residuals are approximately normally distributed.

-   **What This Means**:

    -   The Box-Cox transformation has successfully addressed residual normality issues.

    -   The model now meets the normality assumption of linear regression.

```{r}
AIC(model_transf,model_quad, model)
AIC(model_2nd_order, model_transf_2nd_order)
```
