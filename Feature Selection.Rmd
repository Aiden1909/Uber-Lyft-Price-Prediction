---
title: "R Notebook"
output: html_notebook
---

# Feature Selection

We first remove unnecessary columns like `id` and `product_id` and also remove `source` and `destination` because `distance` already accounts for the relationship between the starting point and the destination.

```{r}
library(dplyr)
training_data <- read.csv('training_data.csv')
test_data <- read.csv(('test_data.csv'))

training_data <- training_data %>% select(-id, -product_id, -source, -destination)
test_data <- test_data %>% select(-id, -product_id, -source, -destination)
```

```{r}
str(training_data)
```

First we would convert all category variables to factor data type

```{r}
training_data$cab_type <- as.factor(training_data$cab_type)
training_data$name <- as.factor(training_data$name)
training_data$time_period <- as.factor(training_data$time_period)
training_data$day_of_week <- as.factor(training_data$day_of_week)

str(training_data)
```

```{r}
par(mfrow = c(2, 2))  # 2x2 layout
subset1 <- training_data[, c("price", "distance", "surge_multiplier")]
pairs(subset1, main = "Subset 1: Price, Distance, Surge Multiplier")

# Subset 2: Numerical weather-related predictors
subset2 <- training_data[, c("price", "temp", "clouds", "rain", "humidity", "wind")]
pairs(subset2, main = "Subset 2: Price, Temp, Clouds, Rain, Humidity, Wind")

# Subset 3: Cab types and ride-specific predictors
subset3 <- training_data[, c("price", "cab_type", "name")]
pairs(subset3, main = "Subset 3: Price, Cab Type, Name")

# Subset 4: Time and day-related predictors
subset4 <- training_data[, c("price", "peak_hour", "weekend", "day_of_week")]
pairs(subset4, main = "Subset 4: Price, Peak Hour, Weekend, Day of Week")
```

### Observations from the Plots:

#### Observations from Pairwise Scatterplots:

1.  **Subset 1: Price, Distance, Surge Multiplier**

    -   A clear **positive relationship** is observed between `price` and `distance`.

    -   **Surge multiplier** also shows a noticeable effect on price, with higher surge multipliers correlating with higher prices.

2.  **Subset 2: Price, Temp, Clouds, Rain, Humidity, Wind**

    -   **Weather variables (temp, clouds, humidity, rain, wind)** seem to have weaker relationships with `price`.

    -   Slight variability can be seen for `rain`, indicating that rainy conditions might influence prices to a small extent.

3.  **Subset 3: Price, Cab Type, Name**

    -   **Ride types (`name`)** show distinct price distributions, with clear separation among categories (e.g., economy vs. luxury).

    -   **Cab type (`Lyft` vs. `Uber`)** also displays visible differentiation in price ranges.

4.  **Subset 4: Price, Peak Hour, Weekend, Day of Week**

    -   Both **`peak_hour`**, **`weekend`** and **`day_of_week`** seem to have subtle effects on price, though the trends are not strongly evident in the scatterplots.

Training on full model first

```{r}
model <- lm(price ~ ., data = training_data)
summary(model)
```

### Key Observations:

1.  **Significant Predictors**:

    -   **Distance**: The coefficient (2.67) indicates that for every unit increase in distance, the price increases by approximately \$2.67, holding other variables constant.

    -   **Cab Type**: "Uber" adds approximately \$14.46 to the base fare compared to Lyft.

    -   **Surge Multiplier**: A unit increase in the surge multiplier increases the price by \$27.14.

    -   **Vehicle Names (e.g., Black SUV, Lux, etc.)**: Significant and show large differences in price, likely due to premium services.

    -   **Time Period (Morning)**: Morning rides have a slightly lower fare (-\$1.62) compared to the reference time period.

2.  **Non-Significant Predictors**:

    -   Weather-related variables (e.g., temperature, clouds, rain, humidity, and wind) are not significant in this model.

    -   Day of the week and peak hour do not significantly influence price.

3.  **Excluded Variables**:

    -   Some variables, such as `nameShared` and `weekend`, were excluded due to singularities, likely because they have little variation or are linearly dependent on other predictors.

4.  **Goodness of Fit**:

    -   The **R-squared** value of 0.9125 and adjusted R-squared of 0.9067 indicate that the model explains over 90% of the variance in price, which is very high.

    -   Residual standard error (2.927) suggests that the typical deviation of observed prices from the fitted prices is around \$2.93.

### Phase 1: Best Subset of Predictors using AIC & BIC

We can see from the above summary that `distance`, `surge_multiplier` , `name` seem to be the important predictors with 95% confidence interval. Next, we want to confirm the best predictors by getting the best AIC & BIC using backward selection.

##### AIC

```{r}
# Perform backward selection using step() with AIC
step(model, direction = "backward", trace = 0)
```

```{r}
aic_model = lm(formula = price ~ distance + surge_multiplier + name + rain, data = training_data)

summary(aic_model)
```

### **Key Observations**:

1.  **Selected Predictors**:

    -   Predictors included in the model:

        -   **Distance**: Strongly significant (p\<2e−16) with a positive coefficient of \~2.69, indicating that price increases with distance.

        -   **Surge Multiplier**: Highly significant (p\<2e−16) and contributes significantly to price (coefficient \~27.17).

        -   **Name**: All levels of the `name` variable are significant, showing that different vehicle types affect price. For example:

            -   "Black SUV" increases price by \~9.55 compared to the reference level.

            -   "Lyft" and "Shared" decrease price significantly compared to the reference.

        -   **Rain**: Marginally significant (p=0.08758), with a positive coefficient (\~6.00), indicating that rainy weather could slightly increase the price.

2.  **Model Fit**:

    -   **R-squared = 0.9092**:

        -   The model explains 90.92% of the variance in the price, which is excellent.

    -   **Adjusted R-squared = 0.9066**:

        -   Adjusted for the number of predictors, this value remains high, indicating that the model is not overfitting.

    -   **Residual Standard Error = 2.928**:

        -   The average deviation of observed prices from the predicted prices is around \$2.93.

3.  **Significance of Predictors**:

    -   Most predictors are highly significant (p\<0.001).

    -   Rain is marginally significant and could be reconsidered for inclusion depending on your objectives.

##### BIC

```{r}
step(model, direction = "backward", trace = 0, k = log(nrow(training_data)))
```

```{r}
bic_model = lm(formula = price ~ distance + surge_multiplier + name, data = training_data)
summary(bic_model)
```

### **Key Observations**:

1.  **Significant Predictors**:

    -   **Distance**: Coefficient 2.6844, p\<2e−16:

        -   For every additional unit of distance, the price increases by approximately \$2.68, holding other variables constant.

    -   **Surge Multiplier**: Coefficient 27.1617, p\<2e−16:

        -   A unit increase in the surge multiplier leads to an increase in price by approximately \$27.16.

    -   **Vehicle Type (Name)**:

        -   Vehicle types significantly impact price compared to the reference level (likely one of the categories, e.g., "Black").

        -   For example:

            -   "Black SUV" increases the price by \~\$9.58.

            -   "Lyft" decreases the price by \~\$11.78.

2.  **Goodness of Fit**:

    -   **R-squared** = 0.9087:

        -   The model explains 90.87% of the variance in price, which is excellent.

    -   **Adjusted R-squared** = 0.9063:

        -   The adjusted value remains high, confirming that adding predictors doesn’t overly complicate the model.

    -   **Residual Standard Error (RSE)** = 2.934:

        -   On average, predictions deviate from actual prices by \~\$2.93.

3.  **Model Simplicity**:

    -   Excluding **rain** from the predictors has not significantly reduced the explanatory power (compare adjusted R-squared from previous models).

### Phase 2: Anova Test

Both AIC and BIC models are promising, so we will compare them using ANOVA and check whether extra predictors in AIC are significant or not.

```{r}
anova(bic_model, aic_model)
```

1.  **F-statistic and p-value**:

    -   F = 2.9301

    -   p-value = 0.08758

    -   The p-value is greater than 0.05, suggesting that the addition of `rain` does not significantly improves the model at a 5% significance level.

### Interpretation:

The results indicate that including `rain` as predictor does not improve the model's explanatory power.

Hence we will go with BIC model.

```{r}
library(car)
vif(bic_model)
```

Next we will add an interaction term and check if its significant or not

```{r}
model_int = lm(price ~ distance + surge_multiplier + name + distance:surge_multiplier, training_data)
summary(model_int)
```

### **Key Observations**:

1.  **Significant Predictors**:

    -   **Distance**: Coefficient = -7.1505:

        -   Surprisingly, the main effect of distance is negative when the interaction is included. This suggests the interaction term dominates the influence of distance.

    -   **Surge Multiplier**: Coefficient = 1.2779:

        -   The main effect of surge multiplier is not significant (p=0.7336), likely due to its interaction with distance.

    -   **Distance:Surge Multiplier**: Coefficient = 9.7187:

        -   Highly significant (p\<2e−16), indicating a strong combined effect of distance and surge multiplier on price.

        -   For every unit increase in both `distance` and `surge_multiplier`, the price increases by approximately \$9.72.

    -   **Name**: All levels of the `name` variable are highly significant, with similar effects as observed in previous models.

2.  **Model Fit**:

    -   **R-squared = 0.9179**:

        -   The model explains 91.79% of the variance in price.

    -   **Adjusted R-squared = 0.9155**:

        -   The adjusted R-squared value indicates the interaction term contributes meaningfully without overfitting.

    -   **Residual Standard Error = 2.785**:

        -   The predictions deviate from actual prices by \~\$2.79 on average.

3.  **Interpretation of Interaction**:

    -   The interaction term suggests the effect of `distance` on price depends on the `surge_multiplier`, and vice versa.

    -   For example:

        -   At higher surge multipliers, the influence of distance on price becomes more pronounced.

### **Evaluation**:

1.  **Advantages**:

    -   The inclusion of the interaction term improves model fit, as indicated by higher R-squared and lower residual standard error.

    -   This model better captures the dynamics between `distance` and `surge_multiplier`.

2.  **Disadvantages**:

    -   The main effects of `distance` and `surge_multiplier` are less interpretable due to the inclusion of the interaction term.

    -   Adding interaction terms increases model complexity, which may lead to overfitting.

```{r}
anova(bic_model,model_int)
```

### **Key Takeaways:**

1.  **Significance of the Interaction Term**:

    -   The F-statistic for the comparison is **54.119**, with a **p-value of** 8.113×10−13.

    -   This indicates that the interaction term **significantly improves the model** at any conventional significance level (p\<0.001).

2.  **Reduction in Residual Sum of Squares (RSS)**:

    -   Model 1 (without interaction): RSS=4182.7

    -   Model 2 (with interaction): RSS=3762.8

    -   The addition of the interaction term reduces the residual variance, suggesting that the interaction term captures additional variability in `price` that the base predictors (`distance`, `surge_multiplier`, `name`) alone cannot explain.

```{r}
AIC(bic_model,model_int)
BIC(bic_model,model_int)
```

Hence we decide to keep the interaction term.
