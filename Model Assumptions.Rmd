---
title: "R Notebook"
output: html_notebook
---

# Model Assumptions

```{r}
training_data <- read.csv('training_data.csv')
test_data <- read.csv(('test_data.csv'))

training_data$name <- as.factor(training_data$name)
```

### Residual Plots Comparison

We have finalized `distance` and `surge_multiplier` and `name` as are predictors for our model and have included interaction term. Next we need to see if our data is able to satisfy all assumptions that are needed for fitting into a linear regression model.

```{r}
model = lm(formula = price ~ distance + surge_multiplier + name + distance:surge_multiplier, data = training_data)

par(mfrow = c(1, 2))

plot(resid(model)~fitted(model), col = "grey", pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "Residual plot")
abline(h = 0, col = "darkorange", lwd = 2)

qqnorm(resid(model), main = "Normal Q-Q Plot", col = "darkgrey")
qqline(resid(model), col = "dodgerblue", lwd = 2)
```

```{r}
library(lmtest)
library(stats)
bptest(model)
shapiro.test(residuals(model))
```

### Observations:

1.  **Residual vs. Fitted Plot:**

    -   The plot shows a distinct funnel shape, suggesting heteroscedasticity. The variance of residuals increases with fitted values, which violates the assumption of homoscedasticity (constant variance).

2.  **Normal Q-Q Plot:**

    -   Residuals deviate from the theoretical line, especially in the tails. This indicates that the residuals are not normally distributed.

3.  **Breusch-Pagan Test:**

    -   The p-value is extremely small (1.75e-06), confirming heteroscedasticity.

4.  **Shapiro-Wilk Test:**

    -   The p-value (\< 2.2e-16) indicates that residuals significantly deviate from normality.

Solution 1: Without Interaction Term

```{r}
model_simple = lm(formula = price ~ distance + surge_multiplier + name, data = training_data)

par(mfrow = c(1, 2))

plot(resid(model_simple)~fitted(model_simple), col = "grey", pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "Residual plot")
abline(h = 0, col = "darkorange", lwd = 2)

qqnorm(resid(model_simple), main = "Normal Q-Q Plot", col = "darkgrey")
qqline(resid(model_simple), col = "dodgerblue", lwd = 2)
```

```{r}
library(lmtest)
library(stats)
bptest(model_simple)
shapiro.test(residuals(model_simple))
```

### Observations:

1.  **Residual vs. Fitted Plot:**

    -   The heteroscedasticity issue remains prominent, as residuals show a clear funnel pattern. This indicates non-constant variance, which violates the homoscedasticity assumption.

2.  **Normal Q-Q Plot:**

    -   Residuals deviate substantially from the normal distribution, especially in the tails. This suggests that the assumption of normality is not met.

3.  **Breusch-Pagan Test:**

    -   The test strongly rejects the null hypothesis of homoscedasticity (p-value = 1.542e-13), confirming significant heteroscedasticity.

4.  **Shapiro-Wilk Test:**

    -   The test confirms that residuals are not normally distributed (p-value \< 2.2e-16).

### Comparison with the Interaction Model:

Removing the interaction term has not resolved the underlying issues in the residuals. This suggests that while the interaction term increased multicollinearity, its inclusion might not be the root cause of assumption violations.

### Solution 2: Removing Outliers

First we check if our data contains any outliers. If there is, then we remove it and try to check for violations again.

```{r}
model_cd = cooks.distance(model)
influential_points = which(model_cd > 4/length(model_cd))
cat("Indices of influential points:", influential_points, "\n")

(training_data[influential_points, c('price','distance','surge_multiplier','name')])
```

### Observations:

1.  **Unusual Values in `distance` or `surge_multiplier`**:

    -   Some points have unusually high distances (e.g., 7.19 at row 255) or surge multipliers (e.g., 2.00 at row 149).

    -   These may skew the regression model.

2.  **Extreme Prices for Specific Ride Types**:

    -   Certain ride types (`UberPool`, `Lux Black XL`, `WAV`) appear multiple times with extreme prices.

3.  **Possible Data Entry Errors**:

    -   Some low `distance` values correspond to disproportionately high prices, which could indicate potential data issues.

```{r}
refined_data = training_data[-influential_points,] 
model_wo_out = lm(price ~ distance + surge_multiplier + name + distance:surge_multiplier, refined_data)

par(mfrow = c(1, 2))

plot(resid(model_wo_out)~fitted(model_wo_out), col = "grey", pch = 20, xlab = "Fitted", ylab = "Residuals", main = "Residual plot") 
abline(h = 0, col = "darkorange", lwd = 2)

qqnorm(resid(model_wo_out), main = "Normal Q-Q Plot", col = "darkgrey") 
qqline(resid(model_wo_out), col = "dodgerblue", lwd = 2)
```

```{r}
print(bptest(model_wo_out))
print(shapiro.test(residuals(model_wo_out)))
```

#### Observations:

1.  **Residual vs. Fitted Plot:**

    -   The heteroscedasticity appears to have reduced compared to the earlier model but is still present, as evidenced by the funnel-like pattern.

    -   Residuals are more evenly distributed around the zero line.

2.  **Normal Q-Q Plot:**

    -   The residuals appear closer to the normal line, indicating improved normality. However, slight deviations in the tails remain.

3.  **Breusch-Pagan Test:**

    -   The p-value (1.576e-07) indicates that heteroscedasticity is still statistically significant.

4.  **Shapiro-Wilk Test:**

    -   The p-value (0.003498) suggests that the residuals still deviate from normality, though the deviation has lessened.

### Insights:

-   Removing outliers has positively impacted the model assumptions, especially for normality.

-   However, heteroscedasticity remains an issue, which may impact the reliability of confidence intervals and hypothesis tests.

### Solution 3: Box-Cox Transformation

Next we go with box cox so that we get a optimal lambda value for our new function of `price`

```{r}
library(MASS)
par(mfrow=c(1,1))
boxcox_model <- boxcox(model_wo_out, lambda = seq(-2, 2, by = 0.1))

optimal_lambda <- boxcox_model$x[which.max(boxcox_model$y)]
cat("Optimal Lambda:", optimal_lambda, "\n")
```

From the plot, we can see 𝝺 = 0.34 approx has the highest log likelihood. So we will take log(price) as our target variable and then try to fit the model.

```{r}
lambda = optimal_lambda
model_transf <- lm(((price^(lambda)-1)/(lambda))~distance + surge_multiplier + name + distance:surge_multiplier,data=refined_data)
par(mfrow = c(1, 2))

plot(resid(model_transf)~fitted(model_transf), col = "grey", pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "Residual plot")
abline(h = 0, col = "darkorange", lwd = 2)

qqnorm(resid(model_transf), main = "Normal Q-Q Plot", col = "darkgrey")
qqline(resid(model_transf), col = "dodgerblue", lwd = 2)
```

```{r}
print(bptest(model_transf))
print(shapiro.test(residuals(model_transf)))
```

#### **Residual Plot:**

-   The residuals are spread fairly evenly around the horizontal line at 0, suggesting that there is no strong pattern or heteroscedasticity. This indicates that the assumptions of linearity and constant variance are reasonably satisfied.

1.  **Breusch-Pagan Test (BP):**

    -   The p-value for the BP test is **0.0525**, which is slightly above the typical threshold of 0.05.

    -   This suggests that there isn't strong evidence of heteroscedasticity in the transformed model, and the assumption of constant variance is mostly met.

2.  **Shapiro-Wilk Normality Test:**

    -   The p-value for the Shapiro-Wilk test is **8.061e-05**, indicating that the residuals deviate from normality. However, given the large sample size, minor deviations from normality are expected and may not heavily impact the validity of your predictions.

### Key Points:

-   The **Box-Cox transformation** has effectively improved the model assumptions, particularly with respect to variance.

-   While the Shapiro-Wilk test indicates non-normality, the Q-Q plot shows only slight deviations, particularly in the tails. This is often acceptable in real-world scenarios where prediction accuracy is prioritized over perfect normality.

```{r}
summary(model_transf)
AIC(model_transf)
```

### Solution 4: 2nd Order

```{r}
model_quad <- lm(price ~ distance + surge_multiplier + I(distance^2) + I(surge_multiplier^2) + name + distance:surge_multiplier, data = refined_data)
summary(model_quad)
```

```{r}
par(mfrow = c(1, 2))

plot(resid(model_quad)~fitted(model_quad), col = "grey", pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "Residual plot")
abline(h = 0, col = "darkorange", lwd = 2)

qqnorm(resid(model_quad), main = "Normal Q-Q Plot", col = "darkgrey")
qqline(resid(model_quad), col = "dodgerblue", lwd = 2)
```

```{r}
print(bptest(model_quad))
print(shapiro.test(residuals(model_quad)))
```

#### The results for the 2nd-order model (quadratic terms) show:

1.  **Residual Plot**: The residuals appear randomly distributed around zero, indicating that the assumption of homoscedasticity might be closer to being satisfied, but the Breusch-Pagan test indicates there is still heteroscedasticity (`p-value = 2.429e-08`).

2.  **Normal Q-Q Plot**: The residuals follow the theoretical quantile line fairly well, except for some deviations in the tails. The Shapiro-Wilk test (`p-value = 0.003394`) indicates that normality is not fully satisfied.

### Evaluation:

-   **Homoscedasticity**: The heteroscedasticity issue persists, though it has likely improved.

-   **Normality**: Normality is still slightly violated but given your sample size (n=477), the central limit theorem reduces its impact on inference.

### 
